{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from analysis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Behavior Evaluation Procedure"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Behavior Examples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "surge_annotations = data.surge_evaluation.annotation_dataframe()\n",
    "seeds = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "for s, b in zip(seeds, behavior):\n",
    "    print(get_example(\n",
    "        data.surge_evaluation,\n",
    "        category.behavior, b, context=0, mark=1,\n",
    "        seed=s,\n",
    "        annotations=surge_annotations\n",
    "    ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pilot Filtering"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def filter_pilot(pilot, qualifications):\n",
    "    pilot_copy = deepcopy(pilot)\n",
    "    work_units_to_keep = set()\n",
    "    for work_unit_id, work_unit in pilot.work_units.items():\n",
    "        if 'likert' not in work_unit.task and 'comparative' not in work_unit.task:\n",
    "            if qualifications[work_unit.worker_id][work_unit.task]:\n",
    "                work_units_to_keep.add(work_unit_id)\n",
    "\n",
    "    for dialogue_id, dialogue in pilot.dialogues.items():\n",
    "        for turn_idx, turn in enumerate(dialogue.turns):\n",
    "            for label, annotations in turn.behavior_annotations.items():\n",
    "                annotations_to_keep = [annot for annot in annotations if annot.work_unit_id in work_units_to_keep]\n",
    "                pilot_copy.dialogues[dialogue_id].turns[turn_idx].behavior_annotations[label] = annotations_to_keep\n",
    "\n",
    "    work_unit_objects_to_keep = {wid: u for wid, u in pilot.work_units.items() if wid in work_units_to_keep}\n",
    "    pilot_copy.work_units = work_unit_objects_to_keep\n",
    "\n",
    "    return pilot_copy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_task(dialogue_id):\n",
    "    if 'personal_information' in dialogue_id:\n",
    "        task = 'personal_information'\n",
    "    elif 'grammar' in dialogue_id:\n",
    "        task = 'interpretability'\n",
    "    else:\n",
    "        task = dialogue_id[:dialogue_id.find('_')]\n",
    "    return task"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove non-Phd+ workers from pilots 0 and 1\n",
    "phd_plus = [\"liyan\", \"zihao\", \"jinho\", \"han\", \"greg\", \"sichang\"]\n",
    "under = [\"sophy\", \"jessica\", \"samir\", \"angela\", \"chen\", \"dan\"]\n",
    "\n",
    "tasks_pilot0 = {u.task for u in data.annotation_pilots[0].work_units.values()}\n",
    "qualifications_pilot0 = {w: {t: True for t in tasks_pilot0} for w in phd_plus}\n",
    "qualifications_pilot0.update({w: {t: False for t in tasks_pilot0} for w in under})\n",
    "grad_filtered_pilot0 = filter_pilot(data.annotation_pilots[0], qualifications_pilot0)\n",
    "\n",
    "tasks_pilot1 = {u.task for u in data.annotation_pilots[1].work_units.values()}\n",
    "qualifications_pilot1 = {w: {t: True for t in tasks_pilot1} for w in phd_plus}\n",
    "qualifications_pilot1.update({w: {t: False for t in tasks_pilot1} for w in under})\n",
    "grad_filtered_pilot1 = filter_pilot(data.annotation_pilots[1], qualifications_pilot1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grad_pilots = [grad_filtered_pilot0, grad_filtered_pilot1, *data.annotation_pilots[2:]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qualifications_pilot0 = {w: {t: True for t in tasks_pilot0} for w in under}\n",
    "qualifications_pilot0.update({w: {t: False for t in tasks_pilot0} for w in phd_plus})\n",
    "undergrad_filtered_pilot0 = filter_pilot(data.annotation_pilots[0], qualifications_pilot0)\n",
    "\n",
    "qualifications_pilot1 = {w: {t: True for t in tasks_pilot1} for w in under}\n",
    "qualifications_pilot1.update({w: {t: False for t in tasks_pilot1} for w in phd_plus})\n",
    "undergrad_filtered_pilot1 = filter_pilot(data.annotation_pilots[1], qualifications_pilot1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "undergrad_pilots = [undergrad_filtered_pilot0, undergrad_filtered_pilot1, *data.annotation_pilots[2:]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pilot0_raw = {\n",
    "    \"use_e_know\", \"cont_e_know\", \"cont_common\", \"use_profile\", \"cont_profile\", \"use_s_ctxt\", \"cont_s_ctxt\", \"use_p_ctxt\", \"cont_p_ctxt\",\n",
    "    \"request\", \"present\", \"ignore_request\", \"ignore_present\",\n",
    "    \"grammar_error\", \"lack_of_sociality\", \"repetition\", \"irrelevant\"\n",
    "}\n",
    "\n",
    "# pilot0 is loaded from excel files, not from json files so it does not use the label standardization process in Research repo: converter.py\n",
    "# missing labels:\n",
    "#   use profile (done), contradict profile (done)\n",
    "#   ignore request (done), ignore present (done)\n",
    "# pilot0 is fully captured in data.json now! (9/14/2022)\n",
    "\n",
    "\n",
    "pilot1_raw = {\n",
    "    \"use_profile\", \"cont_profile\", \"cont_s_ctxt\", \"cont_p_ctxt\", \"redundant\",\n",
    "    \"This response is uninterpretable\",\n",
    "    \"exhibits antisocial behavior\",\n",
    "    \"This response contradicts common knowledge.\"\n",
    "\n",
    "    \"Yes, SPEAKER_X is asking a question or making a request.\",\n",
    "    \"No, SPEAKER_X is just sharing something with SPEAKER_Y.\",\n",
    "    \"Yes, SPEAKER_X is asking for SPEAKER_Y to elaborate on the ideas presented in the previous turn.\",\n",
    "    \"No, SPEAKER_X changes to a different talking point, discussion, or topic.\",\n",
    "    \"Yes, SPEAKER_X is ONLY building on, exploring, or responding to what SPEAKER_Y said in the previous turn.\",\n",
    "    \"Yes, the new talking point is relevant to the current discussion, OR appropriately transitions to a new topic.\",\n",
    "    \"No, introducing the new talking point is abrupt and interrupts the current discussion.\",\n",
    "    \"Yes, SPEAKER_X or acknowledges what SPEAKER_Y just said, OR her response implies that she understood what SPEAKER_Y just said.\",\n",
    "    \"Yes, SPEAKER_X directly responds to or acknowledges what SPEAKER_Y just said, OR SPEAKER_X's response implies that she understood what SPEAKER_Y just said.\",\n",
    "    \"No, SPEAKER_X ignored SPEAKER_Y.\",\n",
    "    \"Yes, what SPEAKER_X said does not necessarily require an acknowledgement or follow-up from SPEAKER_Y.\",\n",
    "    \"No, there is an unspoken expectation that SPEAKER_Y responds to or acknowledges what SPEAKER_X said this turn.\",\n",
    "\n",
    "    \"SPEAKER_X's response incorporates or assumes a fact.\",\n",
    "    \"Yes, I know for sure the fact is accurate.\",\n",
    "    \"The fact is accurate; a credible source verified the fact in my search.\",\n",
    "    \"No, I know for sure the fact is inaccurate, false, or highly implausible.\",\n",
    "    \"The fact is inaccurate; a credible source falsified the fact or revealed that it is highly implausible.\",\n",
    "    \"It is misleading for SPEAKER_X to claim or assume the fact, because there is no way that SPEAKER_X or anyone else has tested whether the fact is accurate.\",\n",
    "    \"My search revealed multiple credible sources that disagreed about whether the fact was true.\",\n",
    "    \"I couldn't find enough credible evidence in my search to either verify or falsify the fact.\"\n",
    "}\n",
    "\n",
    "pilot2_raw = {\n",
    "    \"use_profile\", \"cont_profile\", \"cont_s_ctxt\", \"cont_p_ctxt\", \"redundant\",\n",
    "    \"This response contradicts common knowledge.\",\n",
    "    \"This response exhibits antisocial behavior.\",\n",
    "    \"This response is uninterpretable\",\n",
    "\n",
    "    \"Yes, SPEAKER_X directly responds to or acknowledges what SPEAKER_Y just said, OR SPEAKER_X's response implies that she understood what SPEAKER_Y just said\",\n",
    "    \"No, SPEAKER_X ignored SPEAKER_Y.\",\n",
    "    \"Not applicable, what SPEAKER_Y just said does not require a response or acknowledgement from SPEAKER_X.\",\n",
    "    \"Yes, SPEAKER_X is asking a question or making a request.\",\n",
    "    \"No, SPEAKER_X is just sharing something with SPEAKER_Y.\",\n",
    "    \"Yes, the response is relevant to the current discussion, OR appropriately transitions to a new talking point.\",\n",
    "    \"No, the response feels abrupt and interrupts the current discussion.\",\n",
    "    \"Yes, SPEAKER_X is asking for SPEAKER_Y to elaborate on the ideas presented in the previous turn.\",\n",
    "    \"Yes, SPEAKER_X is ONLY responding to, building on, or further exploring what SPEAKER_Y said in the previous turn.\",\n",
    "    \"No, SPEAKER_X changes to a different talking point, discussion, or topic.\",\n",
    "\n",
    "    \"SPEAKER_X's response incorporates or assumes at least one fact.\",\n",
    "    \"Yes, I know for sure ALL facts are accurate.\",\n",
    "    \"No, I know for sure that one of the facts is inaccurate, false, or highly implausible.\",\n",
    "    \"It is misleading for SPEAKER_X to claim or assume one of the facts, because there is no way that SPEAKER_X knows whether that fact is accurate.\",\n",
    "    \"ALL facts are accurate; a credible source verified the facts in my search.\",\n",
    "    \"One of the facts is inaccurate; a credible source falsified the fact or revealed that it is highly implausible.\",\n",
    "    \"My search revealed multiple credible sources that disagreed about whether one of the facts was true.\",\n",
    "    \"I couldn't find enough credible evidence in my search to either verify or falsify one of the facts.\"\n",
    "}\n",
    "\n",
    "# pilot1 is missing use profile and contradict profile labels after processing of Research repo: converter.py\n",
    "# pilot1 is fully captured in data.json now! (9/14/2022)\n",
    "\n",
    "# pilot3 was missing use profile and contradict profile labels\n",
    "# pilot3 is fully captured in data.json now! (9/14/2022)\n",
    "\n",
    "full_raw = {\n",
    "    \"cont_s_ctxt\", \"cont_p_ctxt\", \"redundant\",\n",
    "    \"This response contradicts common knowledge.\",\n",
    "    \"This response exhibits antisocial behavior.\",\n",
    "    \"This response is uninterpretable\",\n",
    "\n",
    "    \"Yes, SPEAKER_X directly responds to or acknowledges what SPEAKER_Y just said, OR SPEAKER_X's response implies that she understood what SPEAKER_Y just said.\",\n",
    "    \"No, SPEAKER_X ignored SPEAKER_Y.\",\n",
    "    \"Not applicable, what SPEAKER_Y just said does not require a response or acknowledgement from SPEAKER_X.\",\n",
    "    \"Yes, SPEAKER_X is changing the topic of the conversation.\",\n",
    "    \"No, SPEAKER_X  is introducing a new talking point but it is still within the current topic of conversation.\",\n",
    "    \"No, SPEAKER_X is ONLY responding to, building on, or further exploring what SPEAKER_Y said in the previous turn.\",\n",
    "    \"Yes, the response naturally continues the current discussion with relevant questions or ideas, OR the response appropriately transitions to a new discussion if the current discussion has reached a natural conclusion.\",\n",
    "    \"No, the response feels abrupt, and interrupts the current discussion because it is irrelevant\",\n",
    "\n",
    "    \"SPEAKER_X's response incorporates or assumes at least one fact.\",\n",
    "    \"Yes, I know for sure ALL facts are accurate.\",\n",
    "    \"No, I know for sure that one of the facts is inaccurate, false, or highly implausible.\",\n",
    "    \"It is misleading for SPEAKER_X to claim or assume one of the facts, because there is no way that SPEAKER_X knows whether that fact is accurate.\",\n",
    "    \"ALL facts are accurate; a credible source verified the facts in my search.\",\n",
    "    \"One of the facts is inaccurate; a credible source falsified the fact or revealed that it is highly implausible.\",\n",
    "    \"My search revealed multiple credible sources that disagreed about whether one of the facts was true.\",\n",
    "    \"I couldn't find enough credible evidence in my search to either verify or falsify one of the facts.\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pilot Work Distributions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def distributions(evaluations):\n",
    "    summaries = []\n",
    "    for evaluation in evaluations:\n",
    "        # number of dialogues\n",
    "        num_dia = len(evaluation.dialogues.keys())\n",
    "        # number of annotators per dialogue\n",
    "        annotation_counts = set()\n",
    "        for _, dialogue in evaluation.dialogues.items():\n",
    "            for turn in dialogue.turns:\n",
    "                for label, annotations in turn.behavior_annotations.items():\n",
    "                    annotation_counts.add(len(annotations))\n",
    "        annot_per_dia = ', '.join([str(c) for c in annotation_counts])\n",
    "        # number of annotators\n",
    "        annotators = {unit.worker_id for unit in evaluation.work_units.values()}\n",
    "        summaries.append([num_dia, annot_per_dia, ', '.join(annotators), len(annotators)])\n",
    "\n",
    "    sum_df = pd.DataFrame.from_records(summaries)\n",
    "    sum_df.set_axis(\n",
    "        [\"Dialogues\", \"Annotators per Dialogue\", \"Annotators List\", \"Annotators\"],\n",
    "        inplace=True, axis=1\n",
    "    )\n",
    "    return sum_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Behavior Annotation Pilot Agreements"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@to_file\n",
    "def agreement_summaries(evaluations):\n",
    "    summaries = []\n",
    "    for evaluation in evaluations:\n",
    "        annotations = evaluation.annotation_dataframe()\n",
    "        agreement = agreement_dataframe(annotations, ci=False, k=100, dropna=False)\n",
    "        macros = agreement.dropna().mean()\n",
    "        summaries.append(macros)\n",
    "    sum_df = pd.concat(summaries, axis=1).transpose()\n",
    "    sum_df.set_axis(\n",
    "        [stat.kripp_alpha, stat.n],\n",
    "        inplace=True, axis=1\n",
    "    )\n",
    "    # sum_df.drop('x', axis=1, inplace=True)\n",
    "    return sum_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# undergraduates\n",
    "distributions(undergrad_pilots)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agreement_summaries(undergrad_pilots)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# graduates\n",
    "distributions(grad_pilots)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agreement_summaries(grad_pilots)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# all\n",
    "distributions(data.annotation_pilots)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agreement_summaries(data.annotation_pilots)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Screened Pilot Agreements"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "screening_threshold = {\n",
    "    'commonsense': 2,\n",
    "    'consistency': 2,\n",
    "    'empathy': 2,\n",
    "    'interpretability': 1,\n",
    "    'knowledge': 2,\n",
    "    'personal_information': 2,\n",
    "    'sociality': 1,\n",
    "    'transitions': 2\n",
    "}\n",
    "\n",
    "def check_onboarding(project):\n",
    "    workers = {u.worker_id for u in project.work_units.values()}\n",
    "    tasks = {get_task(dialogue_id) for dialogue_id in project.dialogues.keys()}\n",
    "\n",
    "    qualifications = {w: {t: True for t in tasks} for w in workers}\n",
    "    for dialogue_id, onboarding_dialogue in project.dialogues.items():\n",
    "        if '_2' in dialogue_id:\n",
    "            task = get_task(dialogue_id)\n",
    "            for attempt in onboarding_dialogue.attempts:\n",
    "                worker = project.work_units[attempt.work_unit_id].worker_id\n",
    "                if not len(attempt.mistakes) <= screening_threshold[task]:\n",
    "                    qualifications[worker][task] = False\n",
    "    return qualifications\n",
    "\n",
    "lab_pilot = data.annotation_pilots[2]\n",
    "lab_pilot_with_training = data.annotation_pilots_onboarding[2]\n",
    "lab_qualifications = check_onboarding(lab_pilot_with_training)\n",
    "\n",
    "student_pilot = data.annotation_pilots[3]\n",
    "student_pilot_with_training = data.annotation_pilots_onboarding[3]\n",
    "student_qualifications = check_onboarding(student_pilot_with_training)\n",
    "\n",
    "surge_pilot = data.annotation_pilots[4]\n",
    "surge_pilot_with_training = data.annotation_pilots_onboarding[4]\n",
    "surge_qualifications = check_onboarding(surge_pilot_with_training)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make new evaluation that excludes assignments from failed workers\n",
    "\n",
    "filtered_lab_pilot = filter_pilot(lab_pilot, lab_qualifications)\n",
    "filtered_student_pilot = filter_pilot(student_pilot, student_qualifications)\n",
    "filtered_surge_pilot = filter_pilot(surge_pilot, surge_qualifications)\n",
    "agreement_summaries([filtered_lab_pilot, filtered_student_pilot, filtered_surge_pilot])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Behavior Annotation Pilot Screening"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# across_evaluations(\n",
    "#     data.annotation_pilots_onboarding[2:5],\n",
    "#     screening_rates_by_label,\n",
    "#     load='results/annotation_pilot_screening'\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Krippendorf's alpha Verifications"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from analysis import krippendorff\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([\n",
    "    [1, np.nan],\n",
    "    [1, np.nan],\n",
    "    [0, np.nan],\n",
    "    [0, np.nan],\n",
    "    [1, 1],\n",
    "    [1, 1],\n",
    "    [1, 1],\n",
    "    [1, 0],\n",
    "    [1, 0],\n",
    "    [1, 0],\n",
    "    [1, 0]\n",
    "])\n",
    "\n",
    "print(krippendorff.alpha(x.T, level_of_measurement='ordinal'))\n",
    "\n",
    "x = np.array([\n",
    "    [1, 1],\n",
    "    [1, 1],\n",
    "    [1, 1],\n",
    "    [1, 0],\n",
    "    [1, 0],\n",
    "    [1, 0],\n",
    "    [1, 0]\n",
    "])\n",
    "\n",
    "print(krippendorff.alpha(x.T, level_of_measurement='ordinal'))\n",
    "\n",
    "# .alpha ignores cases where only one annotation is available for unit! Good, don't need to do anything special"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}