{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "surge_annotations = data.surge_evaluation.annotation_dataframe()\n",
    "surge_annotations_comparative = data.surge_evaluation.comparative_annotation_dataframe()\n",
    "\n",
    "surge_annotations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8 Comprehensive Analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Metric Sensitivity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def t_tests(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    :param df: (bot, data point) x 1 -> score\n",
    "    :return: p values of test on each bot pair (pd.Series)\n",
    "    \"\"\"\n",
    "    bots = set(df.index.get_level_values(0))\n",
    "    bot_pairs = list(combinations(bots, 2))\n",
    "    result = {}\n",
    "    for ba, bb in bot_pairs:\n",
    "        a = df.xs(ba).to_numpy().squeeze()\n",
    "        b = df.xs(bb).to_numpy().squeeze()\n",
    "        t, p = ttest_ind(a, b)\n",
    "        result[(ba, bb)] = p\n",
    "    result_series = pd.Series(result.values(), result)\n",
    "    return result_series\n",
    "\n",
    "get_singly_annotated(surge_annotations).xs(\n",
    "    category.likert_turn,\n",
    "    level=sym.category\n",
    ").groupby(\n",
    "    sym.label\n",
    ").apply(\n",
    "    t_tests\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predictive Validity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "\n",
    "def regressions(df, quality_column_name=None):\n",
    "    \"\"\"\n",
    "    :param df: dialogue x (*features, quality) -> value\n",
    "    :return: *(coef, low, high), mcfadden r^2\n",
    "    \"\"\"\n",
    "    if not quality_column_name:\n",
    "        quality_column_name = df.columns[-1]\n",
    "    qualities = df[quality_column_name]\n",
    "    features = [f for f in df.columns if f != quality_column_name]\n",
    "    model = OrderedModel(qualities, df[features], distr='logit')\n",
    "    results = model.fit()\n",
    "    coefs = {f: results.params[f] for f in features}\n",
    "    prsqrd = results.prsquared\n",
    "    result = {**coefs, stat.mcfad_r2: prsqrd}\n",
    "    return pd.Series(result.values(), result)\n",
    "\n",
    "def dialogue_metrics(ev):\n",
    "    df: pd.DataFrame = ev.annotation_dataframe()\n",
    "    df = get_singly_annotated(df, seed=123)\n",
    "    reindexed = df.reset_index()\n",
    "    items = reindexed[sym.item]\n",
    "    dialogues = [e[0] if isinstance(e, tuple) else e for e in items]\n",
    "    reindexed['dialogue'] = dialogues\n",
    "    reindexed.set_index(\n",
    "        [sym.bot, sym.category, sym.label, 'dialogue', sym.item],\n",
    "        inplace=True, verify_integrity=True\n",
    "    )\n",
    "    ld = reindexed.xs(category.likert_dialogue, level=sym.category)\n",
    "    ld = ld.droplevel(sym.bot).droplevel(sym.item)\n",
    "    ld.columns = ['score']\n",
    "    ldq = ld.xs(scale.quality, level=sym.label)\n",
    "    ldq.columns = ['quality']\n",
    "\n",
    "    lt = reindexed.xs(category.likert_turn, level=sym.category)\n",
    "    lt = lt.groupby([sym.label, 'dialogue']).mean()\n",
    "    lt.columns = ['score']\n",
    "    ltq = lt.xs(scale.quality, level=sym.label)\n",
    "    ltq.columns = ['quality']\n",
    "\n",
    "    be = reindexed.xs(category.behavior, level=sym.category)\n",
    "    be = be.groupby([sym.label, 'dialogue']).mean()\n",
    "    be.columns = ['score']\n",
    "\n",
    "    ds = pd.concat(\n",
    "        [lt, be, ld],\n",
    "        keys=[category.likert_turn, category.behavior, category.likert_dialogue],\n",
    "        names=[sym.category, sym.label, 'dialogue']\n",
    "    )\n",
    "    likert_dialogue_quality_features = ds.join(ldq, on='dialogue')\n",
    "    likert_turn_quality_features = ds.join(ltq, on='dialogue')\n",
    "    return likert_dialogue_quality_features, likert_turn_quality_features\n",
    "\n",
    "@to_file\n",
    "def dialogue_quality_regressions(ev):\n",
    "    ldq, ltq = dialogue_metrics(ev)\n",
    "    groups = ldq.groupby(\n",
    "        [sym.category, sym.label]\n",
    "    )\n",
    "    result = groups.apply(regressions)\n",
    "    return result\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "dialogue_quality_regressions(\n",
    "    data.surge_evaluation,\n",
    "    load='results/dialogue_quality_regressions'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}