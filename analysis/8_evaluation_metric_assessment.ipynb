{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from analysis import *\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from scipy.stats import binom_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "surge_annotations = data.surge_evaluation.annotation_dataframe()\n",
    "surge_annotations_comparative = data.surge_evaluation.comparative_annotation_dataframe()\n",
    "\n",
    "surge_annotations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8 Comprehensive Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Metric Sensitivity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def p_vals(df: pd.DataFrame, test='t', downsample=None):\n",
    "    \"\"\"\n",
    "    :param df: (bot, data point) x 1 -> score\n",
    "    :param test: statistical test function (t for t test, p for prop test, s for sign test)\n",
    "    :param downsample: number of samples ber bot to subsample without replacement for the analysis\n",
    "    :return: p values of test on each bot pair (pd.Series)\n",
    "    \"\"\"\n",
    "    seed = 123\n",
    "    bots = set(df.index.get_level_values(0))\n",
    "    num_bots = len(bots)\n",
    "    bot_pairs = list(combinations(bots, 2))\n",
    "    result = {}\n",
    "    for ba, bb in bot_pairs:\n",
    "        if test == 't':\n",
    "            if downsample:\n",
    "                a = df.xs(ba).sample(downsample, random_state=seed).to_numpy().squeeze()\n",
    "                b = df.xs(bb).sample(downsample, random_state=seed).to_numpy().squeeze()\n",
    "            else:\n",
    "                a = df.xs(ba).to_numpy().squeeze()\n",
    "                b = df.xs(bb).to_numpy().squeeze()\n",
    "            t, p = ttest_ind(a, b, equal_var=False)\n",
    "        elif test == 'p':\n",
    "            if downsample:\n",
    "                a = df.xs(ba).sample(downsample, random_state=seed).to_numpy().squeeze()\n",
    "                b = df.xs(bb).sample(downsample, random_state=seed).to_numpy().squeeze()\n",
    "            else:\n",
    "                a = df.xs(ba).to_numpy().squeeze()\n",
    "                b = df.xs(bb).to_numpy().squeeze()\n",
    "            z, p = proportions_ztest(count=[\n",
    "                sum(a), sum(b)\n",
    "            ], nobs=[\n",
    "                len(a), len(b)\n",
    "            ])\n",
    "        elif test == 's':\n",
    "            # sign test\n",
    "            comp_data = df.xs((ba, bb), level=[sym.bot, sym.bot_cmp])\n",
    "            if downsample:\n",
    "                comp_data = comp_data.sample(downsample, random_state=seed)\n",
    "            a = comp_data.to_numpy().squeeze() == 1\n",
    "            b = comp_data.to_numpy().squeeze() == -1\n",
    "            p = binom_test(sum(a), sum(a)+sum(b), p=0.5)\n",
    "        else:\n",
    "            raise ValueError('invalid arg for param \"test\"')\n",
    "        result[(ba, bb)] = p\n",
    "    result_series = pd.Series(result.values(), result)\n",
    "    return result_series\n",
    "\n",
    "@to_file\n",
    "def p_values_comparing_bots(evaluation, downsample=None):\n",
    "    comp_annotations = get_singly_annotated(evaluation.comparative_annotation_dataframe(), seed=123)\n",
    "    annotations = get_singly_annotated(evaluation.annotation_dataframe(), seed=123)\n",
    "    prop_annotations = annotations.xs(\n",
    "        category.behavior, level=sym.category, drop_level=False\n",
    "    )\n",
    "    mean_annotations = annotations.drop(\n",
    "        index=category.behavior, level=sym.category\n",
    "    ).drop(\n",
    "        index=category.comparative, level=sym.category\n",
    "    )\n",
    "    mean_ps = mean_annotations.groupby(\n",
    "        [sym.category, sym.label]\n",
    "    ).apply(lambda x: p_vals(x, test='t', downsample=downsample))\n",
    "    prop_ps = prop_annotations.groupby(\n",
    "        [sym.category, sym.label]\n",
    "    ).apply(lambda x: p_vals(x, test='p', downsample=downsample))\n",
    "    comp_groups = comp_annotations.groupby(sym.label)\n",
    "    comp_ps = comp_groups.apply(lambda x: p_vals(x, test='s', downsample=downsample))\n",
    "    comp_ps = pd.concat({category.comparative: comp_ps}, names=[sym.category])\n",
    "    result = pd.concat([prop_ps, mean_ps, comp_ps], axis=0)\n",
    "    return result\n",
    "\n",
    "p_values_comparing_bots(data.surge_evaluation, downsample=32, load='results/p_values_comparing_bots_downsampled').round(4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p_values_comparing_bots(data.surge_evaluation, load='results/t_test_p_values_comparing_bots').round(4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predictive Validity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "from statsmodels.regression.linear_model import OLS as LinearModel\n",
    "from statsmodels.discrete.discrete_model import Logit as LogisticModel\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "\n",
    "def all_dialogue_metrics(data):\n",
    "    static: pd.DataFrame = data.surge_evaluation.annotation_dataframe()\n",
    "    static = get_singly_annotated(static, seed=123)\n",
    "    reindexed = static.reset_index()\n",
    "    items = reindexed[sym.item]\n",
    "    dialogues = [e[0] if isinstance(e, tuple) else e for e in items]\n",
    "    reindexed['dialogue'] = dialogues\n",
    "    reindexed.set_index(\n",
    "        [sym.bot, sym.category, sym.label, 'dialogue', sym.item],\n",
    "        inplace=True, verify_integrity=True\n",
    "    )\n",
    "    ld = reindexed.xs(category.likert_dialogue, level=sym.category)\n",
    "    ld = ld.droplevel(sym.bot).droplevel(sym.item)\n",
    "    ld.columns = ['score']\n",
    "    ldq = ld.xs(scale.quality, level=sym.label)\n",
    "    ldq.columns = ['quality']\n",
    "\n",
    "    lt = reindexed.xs(category.likert_turn, level=sym.category)\n",
    "    lt = lt.groupby([sym.label, 'dialogue']).mean()\n",
    "    lt.columns = ['score']\n",
    "    ltq = lt.xs(scale.quality, level=sym.label)\n",
    "    ltq.columns = ['quality']\n",
    "\n",
    "    be = reindexed.xs(category.behavior, level=sym.category)\n",
    "    be = be.groupby([sym.label, 'dialogue']).mean()\n",
    "    be.columns = ['score']\n",
    "\n",
    "    interactive = data.dialogue_collection.annotation_dataframe()\n",
    "    idq = interactive.xs((category.likert_dialogue, scale.quality), level=(sym.category, sym.label))\n",
    "    idq = idq.droplevel(0)\n",
    "\n",
    "    ds = pd.concat(\n",
    "        [lt, be, ld],\n",
    "        keys=[category.likert_turn, category.behavior, category.likert_dialogue],\n",
    "        names=[sym.category, sym.label, 'dialogue']\n",
    "    )\n",
    "    likert_dialogue_quality_features = ds.join(ldq, on='dialogue')\n",
    "    likert_turn_quality_features = ds.join(ltq, on='dialogue')\n",
    "    interactive_dialogue_quality_features = ds.join(idq, on='dialogue')\n",
    "    interactive_dialogue_quality_features.columns = ['score', 'quality']\n",
    "\n",
    "    interactive_comparisons = data.dialogue_collection.comparative_annotation_dataframe()\n",
    "    surge_comparisons = get_singly_annotated(data.surge_evaluation.comparative_annotation_dataframe(), seed=123)\n",
    "    compared_dialogues = surge_comparisons.index.get_level_values('dialogues')\n",
    "    unique_compared_dialogues = {tuple(x) for x in {frozenset(y) for y in compared_dialogues}}\n",
    "    comparison_map = dict(unique_compared_dialogues)\n",
    "    compared_selector = [\n",
    "        pair in unique_compared_dialogues\n",
    "        for pair in interactive_comparisons.index.get_level_values('dialogues')\n",
    "    ]\n",
    "    comparative: pd.DataFrame = interactive_comparisons.loc[compared_selector, :]\n",
    "    compared_selector = [\n",
    "        pair in unique_compared_dialogues\n",
    "        for pair in surge_comparisons.index.get_level_values('dialogues')\n",
    "    ]\n",
    "    surge_comparisons: pd.DataFrame = surge_comparisons.loc[compared_selector, :]\n",
    "    comparative_quality = comparative.xs(scale.quality, level=sym.label)\n",
    "    comparative_quality.index = [first for _, _, (first, second) in comparative_quality.index.values]\n",
    "    comparative_quality.columns = ['quality']\n",
    "    surge_comparisons.index = pd.MultiIndex.from_arrays(\n",
    "        list(zip(*[\n",
    "            (category.comparative, label, left)\n",
    "            for _, _, label, (left, right) in surge_comparisons.index.values\n",
    "        ])),\n",
    "        names=[sym.category, sym.label, 'dialogue']\n",
    "    )\n",
    "    surge_comparisons.columns = ['score']\n",
    "    filtered_ds = ds.loc[[(c, l, d) for c, l, d in ds.index.values if d in comparison_map]]\n",
    "    compared_features = ds.loc[[(c, l, comparison_map[d]) for c, l, d in filtered_ds.index.values]]\n",
    "    comparative_features = filtered_ds.to_numpy() - compared_features.to_numpy()\n",
    "    filtered_ds['diff'] = comparative_features.squeeze().tolist()\n",
    "    del filtered_ds['score']\n",
    "    filtered_ds.columns = ['score']\n",
    "    filtered_ds = pd.concat([filtered_ds, surge_comparisons], axis=0)\n",
    "    comparative_quality_features = filtered_ds.join(comparative_quality, on='dialogue')\n",
    "    icq = comparative_quality_features\n",
    "    icq = icq[icq['quality'] != 0]\n",
    "    icq.loc[:,'quality'] = (icq['quality'] > 0).astype(int)\n",
    "\n",
    "    return (\n",
    "        likert_dialogue_quality_features,\n",
    "        likert_turn_quality_features,\n",
    "        icq,\n",
    "        interactive_dialogue_quality_features\n",
    "    )\n",
    "\n",
    "all_dialogue_metrics(data)\n",
    "\n",
    "def regressions(df, quality_column_name=None, model='linear'):\n",
    "    \"\"\"\n",
    "    :param df: dialogue x (*features, quality) -> value\n",
    "    :return: *(coef, low, high), mcfadden r^2\n",
    "    \"\"\"\n",
    "    if not quality_column_name:\n",
    "        quality_column_name = df.columns[-1]\n",
    "    qualities = df[quality_column_name]\n",
    "    features = [f for f in df.columns if f != quality_column_name]\n",
    "    if model == 'ordinal':\n",
    "        model = OrderedModel(qualities, df[features], distr='logit')\n",
    "        results = model.fit()\n",
    "        coefs = {f: results.params[f] for f in features}\n",
    "        prsqrd = results.prsquared\n",
    "        result = {stat.mcfad_r2: prsqrd, stat.p_of_llr_test: results.llr_pvalue}\n",
    "    elif model == 'linear':\n",
    "        x = add_constant(df[features])\n",
    "        y = qualities\n",
    "        model = LinearModel(y, x)\n",
    "        results = model.fit()\n",
    "        coefs = {f: results.params[f] for f in features}\n",
    "        rsquared = results.rsquared\n",
    "        result = {**coefs, stat.r2: rsquared, stat.p_of_f_test: results.f_pvalue}\n",
    "    elif model == 'logistic':\n",
    "        x = add_constant(df[features])\n",
    "        y = qualities.astype(bool)\n",
    "        model = LogisticModel(y, x)\n",
    "        results = model.fit()\n",
    "        coefs = {f: results.params[f] for f in features}\n",
    "        prsqrd = results.prsquared\n",
    "        result = {**coefs, stat.mcfad_r2: prsqrd, stat.p_of_llr_test: results.llr_pvalue}\n",
    "    else:\n",
    "        raise ValueError('Param \"model\" must be one of {\"linear\", \"ordinal\", \"logistic\"}')\n",
    "    return pd.Series(result.values(), result)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@to_file\n",
    "def dialogue_quality_regressions(data):\n",
    "    ldq, ltq, icq, idq = all_dialogue_metrics(data)\n",
    "    ldq_groups = ldq.groupby([sym.category, sym.label])\n",
    "    ltq_groups = ltq.groupby([sym.category, sym.label])\n",
    "    icq_groups = icq.groupby([sym.category, sym.label])\n",
    "    idq_groups = idq.groupby([sym.category, sym.label])\n",
    "    names = ['Predicted', 'Metric']\n",
    "    linear_compare_result = icq_groups.apply(lambda x: regressions(x, model='logistic'))\n",
    "    linear_compare_result.columns = pd.MultiIndex.from_arrays(\n",
    "        [['Interactive Comparison']*3,\n",
    "        ['LC Coefficient', 'LC Pseudo R-Squared', stat.p_of_llr_test]],\n",
    "        names=names\n",
    "    )\n",
    "    linear_result = ldq_groups.apply(lambda x: regressions(x, model='linear'))\n",
    "    linear_result.columns = pd.MultiIndex.from_arrays(\n",
    "        [['Likert Dialogue Quality']*3,\n",
    "        ['LR Coefficient', 'LR R-Squared', stat.p_of_f_test]],\n",
    "        names=names\n",
    "    )\n",
    "    ordinal_result = ldq_groups.apply(lambda x: regressions(x, model='ordinal'))\n",
    "    ordinal_result.columns = pd.MultiIndex.from_arrays(\n",
    "        [['Likert Dialogue Quality']*2,\n",
    "        ['OR Pseudo R-Squared', stat.p_of_llr_test]],\n",
    "        names=names\n",
    "    )\n",
    "    linear_turn_result = ltq_groups.apply(regressions)\n",
    "    linear_turn_result.columns = pd.MultiIndex.from_arrays(\n",
    "        [['Likert Turn Quality']*3,\n",
    "        ['LR Coefficient', 'LR R-Squared', stat.p_of_f_test]],\n",
    "        names=names\n",
    "    )\n",
    "    interactive_dial_result = idq_groups.apply(regressions)\n",
    "    interactive_dial_result.columns = pd.MultiIndex.from_arrays(\n",
    "        [['Interactive Likert']*3,\n",
    "        ['LR Coefficient', 'LR R-Squared', stat.p_of_f_test]],\n",
    "        names=names\n",
    "    )\n",
    "    result = pd.concat(( linear_compare_result, interactive_dial_result, linear_result, linear_turn_result), axis=1)\n",
    "    return result.round(5)\n",
    "\n",
    "regs = dialogue_quality_regressions(\n",
    "    data,\n",
    "    load='results/dialogue_quality_regressions'\n",
    ")\n",
    "regs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_plot_regs = regs[[(\"Likert Dialogue Quality\", \"LR R-Squared\"), (\"Likert Dialogue Quality\", \"P value of F-test\")]]\n",
    "to_plot_regs = to_plot_regs.drop((\"likert dialogue\", \"quality\"))\n",
    "to_plot_regs = to_plot_regs.reset_index()\n",
    "to_plot_regs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "regs = prettify(regs, to_csv=\"results/paper/predictive_validity.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Incremental Validity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def drop_column_level_duplication(df: pd.DataFrame, columns, levels=None):\n",
    "    if levels is None:\n",
    "        levels = list(range(len(columns)))\n",
    "    level_columns = df.xs(columns, axis=1, level=levels)\n",
    "    unique = level_columns.iloc[:,0].to_frame()\n",
    "    unique.columns = [columns]\n",
    "    dropped = df.drop(columns=columns, level=levels)\n",
    "    result = pd.concat([dropped, unique], axis=1)\n",
    "    return result\n",
    "\n",
    "def multivariate_regression(df: pd.DataFrame, model='linear'):\n",
    "    def apply_regressions(df: pd.DataFrame):\n",
    "        unstacked = df.unstack([sym.category, sym.label])\n",
    "        unstacked = drop_column_level_duplication(unstacked, 'quality', 0)\n",
    "        results = regressions(unstacked, quality_column_name='quality', model=model)\n",
    "        return results\n",
    "    result = apply_regressions(df)\n",
    "    result.index = [\n",
    "        (idx[1] if isinstance(idx, tuple) else idx)\n",
    "        for idx in result.index.values\n",
    "    ]\n",
    "    return result.round(5)\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "@to_file\n",
    "def incremental_regression(\n",
    "        df: pd.DataFrame,\n",
    "        categories,\n",
    "        model='linear',\n",
    "        beam=1,\n",
    "        select='backward',\n",
    "        exclusions=(),\n",
    "):\n",
    "    data_points = set(df.index.get_level_values('dialogue'))\n",
    "    num_data_points = len(data_points)\n",
    "    adjust = lambda r2, f: 1 - (1 - r2) * ((num_data_points - 1) / (num_data_points - f))\n",
    "    Step: type = namedtuple('Step', ('r2', 'p', 'feature'))\n",
    "    class Path(list):\n",
    "        def metric(self):\n",
    "            # if len(self) == 0: return 0\n",
    "            # else: return self[-1].llr if len(self) == 1 else self[-1].llr / self[-2].llr\n",
    "            return self.r2\n",
    "        @property\n",
    "        def r2(self):\n",
    "            return (adjust(self[-1].r2, len(self)) if model == 'linear' else self[-1].r2) if self else 0\n",
    "        # @property\n",
    "        # def adj_r2(self):\n",
    "        #     return adjust(self.r2, len(self))\n",
    "        @property\n",
    "        def p(self): return self[-1].p if self else 1\n",
    "        @property\n",
    "        def features(self): return {x.feature for x in self}\n",
    "    r2_name = stat.r2 if model=='linear' else stat.mcfad_r2\n",
    "    p_name = stat.p_of_f_test if model=='linear' else stat.p_of_llr_test\n",
    "    frontier = [Path()]\n",
    "    feature_pool = {\n",
    "        x[:2] for x in df.index.values\n",
    "        if (not (x in exclusions or x[1] in exclusions))\n",
    "        and x[0] in categories\n",
    "    }\n",
    "    for _ in feature_pool:\n",
    "        new_frontier = []\n",
    "        for path in frontier:\n",
    "            for candidate in feature_pool - path.features:\n",
    "                if select == 'forward':\n",
    "                    candidate_features = path.features | {candidate}\n",
    "                elif select == 'backward':\n",
    "                    candidate_features = feature_pool - path.features\n",
    "                else:\n",
    "                    raise ValueError('param select must be one of {\"forward\", \"backward\"}')\n",
    "                row_mask = [\n",
    "                    x[:2] in candidate_features\n",
    "                    and (not (x in exclusions or x[1] in exclusions))\n",
    "                    and x[0] in categories\n",
    "                    for x in df.index.values\n",
    "                ]\n",
    "                candidate_df = df.loc[row_mask, :]\n",
    "                candidate_results = multivariate_regression(candidate_df, model=model)\n",
    "                r2 = candidate_results[r2_name].item()\n",
    "                p = candidate_results[p_name]\n",
    "                new_frontier.append(Path([*path, Step(r2, p, candidate)]))\n",
    "        frontier = sorted(new_frontier, key=lambda x: x.metric(), reverse=True)[:beam]\n",
    "    result = {step.feature: {r2_name: step.r2, p_name: step.p} for step in frontier[0]}\n",
    "    return pd.DataFrame(result.values(), result)\n",
    "\n",
    "\n",
    "ldq, ltq, icq, idq = all_dialogue_metrics(data)\n",
    "regs = incremental_regression(\n",
    "    ldq, (category.likert_turn, category.behavior), beam=1, exclusions=[scale.quality],\n",
    "    load='results/dialogue_incremental_regressions'\n",
    ")\n",
    "regs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "behavior_regs = incremental_regression(\n",
    "    ldq, (category.behavior,), beam=10,\n",
    "    load='results/behavior_incremental_regressions'\n",
    ")\n",
    "behavior_regs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "behavior_regs = incremental_regression(\n",
    "    idq, (category.behavior,), beam=10,\n",
    "    load='results/behavior_incremental_regressions_interactive'\n",
    ")\n",
    "behavior_regs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "behavior_regs_comp = incremental_regression(\n",
    "    icq, (category.behavior,), beam=10, model='logistic',\n",
    "    load='results/behavior_incremental_regressions_comparative'\n",
    ")\n",
    "behavior_regs_comp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "likert_turn_regs = incremental_regression(\n",
    "    ldq, (category.likert_turn,), beam=10, exclusions=[scale.quality],\n",
    "    load='results/likert_turn_incremental_regressions'\n",
    ")\n",
    "likert_turn_regs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "likert_turn_regs = incremental_regression(\n",
    "    idq, (category.likert_turn,), beam=10, exclusions=[scale.quality],\n",
    "    load='results/likert_turn_incremental_regressions_interactive'\n",
    ")\n",
    "likert_turn_regs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "likert_turn_regs_comp = incremental_regression(\n",
    "    icq, (category.likert_turn,), beam=10, model='logistic', exclusions=['quality'],\n",
    "    load='results/likert_turn_incremental_regressions_comparative'\n",
    ")\n",
    "likert_turn_regs_comp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "likert_turn_regs_comp = incremental_regression(\n",
    "    icq, (category.likert_turn,), beam=10, model='logistic', exclusions=[scale.proactive, scale.quality],\n",
    "    load='results/likert_turn_incremental_regressions_comparative_no_proactive_or_quality'\n",
    ")\n",
    "likert_turn_regs_comp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "likert_dialogue_regs = incremental_regression(\n",
    "    ldq, (category.likert_dialogue,), beam=10, exclusions=['quality'],\n",
    "    load='results/likert_dialogue_incremental_regressions'\n",
    ")\n",
    "likert_dialogue_regs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "likert_dialogue_regs = incremental_regression(\n",
    "    idq, (category.likert_dialogue,), beam=10, exclusions=['quality'],\n",
    "    load='results/likert_dialogue_incremental_regressions_interactive'\n",
    ")\n",
    "likert_dialogue_regs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "likert_dialogue_regs_comp = incremental_regression(\n",
    "    icq, (category.likert_dialogue,), beam=10, model='logistic', exclusions=['quality'],\n",
    "    load='results/likert_dialogue_incremental_regressions_comparative'\n",
    ")\n",
    "likert_dialogue_regs_comp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "likert_dialogue_regs_comp = incremental_regression(\n",
    "    icq, (category.likert_dialogue,), beam=10, model='logistic', exclusions=[scale.engaging, scale.quality],\n",
    "    load='results/likert_dialogue_incremental_regressions_comparative_no_engaging_or_quality'\n",
    ")\n",
    "likert_dialogue_regs_comp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "comparative_dialogue_regs_comp = incremental_regression(\n",
    "    icq, (category.comparative,), beam=10, model='logistic', exclusions=[scale.quality],\n",
    "    load='results/comparative_incremental_regressions_comparative'\n",
    ")\n",
    "comparative_dialogue_regs_comp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Table for Paper"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_delta(df):\n",
    "    df['R-Squared'] = (df['Adjusted R-Squared']*100)\n",
    "    df['delta'] = df['R-Squared'].diff()\n",
    "    df['delta'] = df['delta'].map('{:.4f}'.format)\n",
    "    df['R-Squared'] = df['R-Squared'].map('{:.4f}'.format)\n",
    "    df['R-Squared delta'] = df['R-Squared'] + ' (' + df['delta'] + ')'\n",
    "\n",
    "final_behavior_regs = behavior_regs.reset_index().rename({'level_1': 'ABC-Eval'}, axis=1).drop(['level_0', 'P value of F-test'], axis=1)\n",
    "add_delta(final_behavior_regs)\n",
    "\n",
    "final_likert_turn_regs = likert_turn_regs.reset_index().rename({'level_1': 'Likert Turn'}, axis=1).drop(['level_0', 'P value of F-test'], axis=1)\n",
    "add_delta(final_likert_turn_regs)\n",
    "\n",
    "final_likert_dialogue_regs = likert_dialogue_regs.reset_index().rename({'level_1': 'Likert Dialogue'}, axis=1).drop(['level_0', 'P value of F-test'], axis=1)\n",
    "add_delta(final_likert_dialogue_regs)\n",
    "\n",
    "final_behavior_regs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined = pd.concat(\n",
    "    [\n",
    "        final_behavior_regs[['ABC-Eval', 'R-Squared delta']],\n",
    "        final_likert_turn_regs[['Likert Turn', 'R-Squared delta']],\n",
    "        final_likert_dialogue_regs[['Likert Dialogue', 'R-Squared delta']]\n",
    "    ],\n",
    "    axis=1)\n",
    "\n",
    "combined.to_csv('results/paper/incremental_validity.csv', index=False)\n",
    "combined"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
