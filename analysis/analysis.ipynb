{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 476,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "from analysis import *\n",
    "import evaluation_data_definitions as edd\n",
    "\n",
    "import nltk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "surge_annotations_comparative"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def to_file(f):\n",
    "    def fn_to_file(*args, load=None, reload=None, **kwargs):\n",
    "        if load:\n",
    "            return pd.read_csv(load)\n",
    "        result = f(*args, **kwargs)\n",
    "        if reload:\n",
    "            result.to_csv(reload)\n",
    "        return result\n",
    "    return fn_to_file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prettify(df, float_prec=None, col_types=None, sort_by=None, to_csv=None, index=True, header=True):\n",
    "    if col_types:\n",
    "        for col, type in col_types.items():\n",
    "            df[col] = df[col].astype(type)\n",
    "    if sort_by:\n",
    "        df.sort_values(sort_by, ascending=False, inplace=True)\n",
    "    if float_prec:\n",
    "        df = df.round(float_prec)\n",
    "    if to_csv:\n",
    "        df.to_csv(to_csv, float_format=f\"%.{float_prec}f\", header=header, index=index)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@to_file\n",
    "def across_evaluations(annotations, evaluation_fn):\n",
    "    \"\"\"\n",
    "    :param annotations: iterable of annotations df to apply evaluation_fn to\n",
    "    :param evaluation_fn: function (input is annotations df, output is results df)\n",
    "    :return: results dataframe where first index level codes which evaluation (integer id)\n",
    "    \"\"\"\n",
    "    results = [evaluation_fn(annotation) for annotation in annotations]\n",
    "    all_results = pd.concat(results, keys=range(len(results)))\n",
    "    all_results.index.set_names('round', level=0, inplace=True)\n",
    "    return all_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Behavior Evaluation Procedure"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Behavior Examples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_example(\n",
    "        evaluation,\n",
    "        category,\n",
    "        label,\n",
    "        mark,\n",
    "        bot=None,\n",
    "        context=0,\n",
    "        seed=123,\n",
    "        annotations: pd.DataFrame = None\n",
    "):\n",
    "    if annotations is None:\n",
    "        annotations = evaluation.annotation_dataframe()\n",
    "    labels = annotations.xs((category, label), level=(1, 2)).reset_index()\n",
    "    options = labels[labels[0] == mark]\n",
    "    if bot:\n",
    "        options = options[options[sym.bot] == bot]\n",
    "    try:\n",
    "        example = options.sample(1, random_state=seed)\n",
    "    except ValueError:\n",
    "        return f'No samples for {category} {label} {mark} {bot}\\n'\n",
    "    eid = example[sym.item].item()\n",
    "    if isinstance(eid, tuple):\n",
    "        did, tid = eid\n",
    "        turns = evaluation.dialogues[did].turns[max(0, tid-context):tid+1]\n",
    "        botstring = '' if not bot else f'{bot}~~~\\n'\n",
    "        contextstring = ''.join((\n",
    "            (\n",
    "                f'User:  {turn.user_turn}\\n'\n",
    "                f'Sys:   {turn.bot_turn}\\n'\n",
    "            )\n",
    "            for turn in turns[:-1]\n",
    "        ))\n",
    "        turn = turns[-1]\n",
    "        turnstring = (\n",
    "            f'User:  {turn.user_turn}\\n'\n",
    "            f'Sys:   {turn.bot_turn}\\n'\n",
    "            f'Label: {label} = {mark}\\n'\n",
    "        )\n",
    "        return botstring + contextstring + turnstring\n",
    "    else:\n",
    "        dialogue = evaluation.dialogues[eid]\n",
    "        turns = [\n",
    "            turn\n",
    "            for turn_pair in dialogue.turns\n",
    "            for turn in (turn_pair.user_turn, turn_pair.bot_turn)\n",
    "        ]\n",
    "        return '\\n'.join([f'{dialogue.bot}~~~', *turns, f'Label: {label} = {mark}\\n'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seeds = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "for s, b in zip(seeds, behavior):\n",
    "    print(get_example(\n",
    "        data.surge_evaluation,\n",
    "        category.behavior, b, context=0, mark=1,\n",
    "        seed=s,\n",
    "        annotations=surge_annotations\n",
    "    ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Behavior Annotation Pilot Agreements"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def agreement_dataframe(annotations, load=None, reload=None, ci=True):\n",
    "    if load:\n",
    "        return pd.read_csv(load)\n",
    "    doubly_annotated = annotations.iloc[:,:2].dropna().astype(int)\n",
    "    label_groups = doubly_annotated.groupby(level=[sym.category, sym.label])\n",
    "    kappas = label_groups.apply(fleiss_kappa, ci=ci)\n",
    "    alphas = label_groups.apply(krippendorfs_alpha, ci=ci)\n",
    "    agreements = pd.concat((alphas, kappas), axis=1)\n",
    "    if reload:\n",
    "        agreements.to_csv(reload)\n",
    "    return agreements"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def agreement_summaries(evaluations, load=None, reload=None):\n",
    "    if load:\n",
    "        return pd.read_csv(load)\n",
    "    summaries = []\n",
    "    for evaluation in evaluations:\n",
    "        annotations = evaluation.annotation_dataframe()\n",
    "        agreement = agreement_dataframe(annotations, ci=False)\n",
    "        macros = agreement.dropna().mean()\n",
    "        summaries.append(macros)\n",
    "    if reload:\n",
    "        ...\n",
    "    sum_df = pd.concat(summaries, axis=1).transpose()\n",
    "    sum_df.set_axis(\n",
    "        [stat.kripp_alpha, 'x', stat.fleiss_kappa, stat.n],\n",
    "        inplace=True, axis=1\n",
    "    )\n",
    "    sum_df.drop('x', axis=1, inplace=True)\n",
    "    return sum_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# todo - include ALL pilot annotations in agreement calculation (not just double annotation)\n",
    "agreement_summaries(data.annotation_pilots)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Behavior Annotation Pilot Screening"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@to_file\n",
    "def screening_rates_by_label(evaluation: edd.OnboardingEvaluation):\n",
    "    perfs = {}\n",
    "    workers_passed = {}\n",
    "    workers_attempted = {}\n",
    "    for did, dialogue in evaluation.dialogues.items():\n",
    "        for attempt in dialogue.attempts:\n",
    "            work_unit = evaluation.work_units[attempt.work_unit_id]\n",
    "            round = int(did.split('_')[-1])\n",
    "            task = work_unit.task\n",
    "            labels = work_unit.labels\n",
    "            num_mistakes = len(attempt.mistakes)\n",
    "            worker = work_unit.worker_id\n",
    "            accuracy = attempt.performance\n",
    "            perfs.setdefault(task, []).append((num_mistakes, accuracy))\n",
    "            workers_attempted.setdefault(task, set()).add(worker)\n",
    "            if attempt.passed:\n",
    "                workers_passed.setdefault(task, set()).add(worker)\n",
    "    screening = {}\n",
    "    for task, ls in perfs.items():\n",
    "        mistakes, accuracies = zip(*ls)\n",
    "        avg_m = sum(mistakes) / len(mistakes)\n",
    "        avg_a = (\n",
    "            sum(accuracies) / len(accuracies)\n",
    "            if all((a is not None for a in accuracies)) else None\n",
    "        )\n",
    "        n = len(mistakes)\n",
    "        attempted = len(workers_attempted.get(task, ()))\n",
    "        passed = len(workers_passed.get(task, ()))\n",
    "        screening[task] = {\n",
    "            'attempted': attempted, 'passed': passed,\n",
    "            'mistakes': avg_m, 'accuracy': avg_a, 'n': n\n",
    "        }\n",
    "    return pd.DataFrame(screening.values(), screening)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "across_evaluations(\n",
    "    data.annotation_pilots_onboarding[2:4],\n",
    "    screening_rates_by_label,\n",
    "    reload='results/annotation_pilot_screening.csv'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 Model Selection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bot Pilot Summary Statistics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@to_file\n",
    "def interactor_summary_stats(evaluation: edd.Evaluation):\n",
    "    num_dialogues = len(evaluation.dialogues)\n",
    "    mean_turns = (\n",
    "        sum((\n",
    "            2*len(d.turns)\n",
    "            for d in evaluation.dialogues.values()\n",
    "        ))\n",
    "        / num_dialogues\n",
    "    )\n",
    "    user_turn_len = (\n",
    "        sum((\n",
    "            len(nltk.word_tokenize(t.user_turn))\n",
    "            for d in evaluation.dialogues.values()\n",
    "            for t in d.turns\n",
    "        ))\n",
    "        / sum((\n",
    "            len(d.turns)\n",
    "            for d in evaluation.dialogues.values()\n",
    "        ))\n",
    "    )\n",
    "    num_interactors = len({\n",
    "        unit.worker_id\n",
    "        for unit in evaluation.work_units.values()\n",
    "    })\n",
    "    summary = {\n",
    "        'dialogues': num_dialogues,\n",
    "        'mean turns': mean_turns,\n",
    "        'user turn length': user_turn_len,\n",
    "        'interactors': num_interactors,\n",
    "    }\n",
    "    return pd.DataFrame(summary.values(), summary)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "across_evaluations(\n",
    "    data.bot_pilots, interactor_summary_stats,\n",
    "    load='results/bot_pilot_summary.csv'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bot Pilots Likert Quality"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@to_file\n",
    "def evaluate_interactive_likert(annotations):\n",
    "    likert_annotations = annotations.xs(category.likert_dialogue, level=sym.category)\n",
    "    label_groups = likert_annotations.groupby(level=[sym.bot, sym.label])\n",
    "    means = label_groups.apply(mean_and_ci)\n",
    "    return means\n",
    "\n",
    "qdf = evaluate_interactive_likert(\n",
    "    data.bot_pilots[0].annotation_dataframe(),\n",
    "    reload='results/bot_pilot_interactive_likert.csv'\n",
    ").xs(scale.quality, level=sym.label)\n",
    "qdf = prettify(qdf, float_prec=3, col_types={\"n\": \"int\"}, sort_by=\"mean\", to_csv=\"results/paper/bot_pilot_interactive_likert_quality.csv\")\n",
    "qdf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bot Pilot Comparative Quality"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_singly_annotated(df: pd.DataFrame, seed=None):\n",
    "    if len(df.columns) == 1:\n",
    "        return df.astype(int)\n",
    "    previous_state = random.getstate()\n",
    "    random.seed(seed)\n",
    "    df = df.iloc[:,:2]\n",
    "    mask = df[1].isna()\n",
    "    singly_annotated = df.iloc[:,0][mask]\n",
    "    doubly_annotated = df[~mask]\n",
    "    selection = [random.randint(0, 1) for _ in range(len(doubly_annotated))]\n",
    "    indices = list(range(len(doubly_annotated)))\n",
    "    select_annotated = doubly_annotated.values[indices, selection]\n",
    "    select_annotated = pd.DataFrame(select_annotated, index=doubly_annotated.index)\n",
    "    annotations = pd.concat((singly_annotated, select_annotated))\n",
    "    random.setstate(previous_state)\n",
    "    return annotations.astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@to_file\n",
    "def evaluate_comparisons(annotations):\n",
    "    single_annotated = get_singly_annotated(annotations)\n",
    "    prop_dfs = []\n",
    "    for cmp, cmp_label in {-1: 'lose', 0: 'tie', 1: 'win'}.items():\n",
    "        annotated = single_annotated == cmp\n",
    "        annotated = annotated.astype(int)\n",
    "        groups = annotated.groupby(level=[sym.bot, sym.bot_cmp, sym.label])\n",
    "        props = groups.apply(prop_and_ci)\n",
    "        props.rename(columns={stat.proportion: cmp_label}, inplace=True)\n",
    "        prop_dfs.append(props)\n",
    "    result = pd.concat(prop_dfs, axis=1)\n",
    "    prop_dfs = []\n",
    "    for cmp, cmp_label in {-1: 'lose', 0: 'tie', 1: 'win'}.items():\n",
    "        annotated = single_annotated == cmp\n",
    "        annotated = annotated.astype(int)\n",
    "        groups = annotated.groupby(level=[sym.bot, sym.label])\n",
    "        props = groups.apply(prop_and_ci)\n",
    "        props.rename(columns={stat.proportion: cmp_label}, inplace=True)\n",
    "        prop_dfs.append(props)\n",
    "    result_vs_all = pd.concat(prop_dfs, axis=1)\n",
    "    others_idx = {sym.bot_cmp: 'others'}\n",
    "    result_vs_all = result_vs_all.assign(**others_idx)\n",
    "    levels = [sym.bot, sym.bot_cmp, sym.label]\n",
    "    result_vs_all = result_vs_all.set_index(sym.bot_cmp, append=True)\n",
    "    result_vs_all = result_vs_all.reset_index().set_index(levels)\n",
    "    result = pd.concat((result_vs_all, result))\n",
    "    return result\n",
    "\n",
    "cmp_annot = data.bot_pilots[0].comparative_annotation_dataframe()\n",
    "\n",
    "cmp = evaluate_comparisons(\n",
    "    cmp_annot,\n",
    "    reload='results/bot_pilot_interactive_comparative.csv'\n",
    ")\n",
    "cmp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bot Pilot Conversation Examples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seeds = [1, 1, 1, 1, 1, 1, 1]\n",
    "for s, b in zip(seeds, bot):\n",
    "    example = get_example(\n",
    "        data.bot_pilots[0],\n",
    "        category.likert_dialogue, label=scale.quality, bot=b, context=0, mark=1,\n",
    "        seed=s\n",
    "    )\n",
    "    print(example)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 Conversation Collection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Time results to collect conversations\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conversation Data Summary Statistics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = interactor_summary_stats(\n",
    "    data.dialogue_collection,\n",
    "    load='results/conversation_summary_stats.csv'\n",
    ")\n",
    "df = prettify(df, float_prec=3, to_csv=\"results/paper/conversation_data_summary.csv\", index=False, header=False)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Timing results for training and collection (per task)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Worker Group Completed Work"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.surge_evaluation.annotation_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.student_evaluation.annotation_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.mturk_evaluation.annotation_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Worker Group Screening"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "screening = across_evaluations(\n",
    "    [data.annotation_pilots_onboarding[-2], data.student_onboarding, data.mturk_onboarding, data.annotation_pilots_onboarding[-1], data.surge_onboarding],\n",
    "    screening_rates_by_label,\n",
    "    reload='results/evaluation_screening.csv'\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Agreements"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agreements = agreement_dataframe(\n",
    "    surge_annotations, load='results/surge_agreements.csv'\n",
    ")\n",
    "agreements = prettify(agreements, float_prec=3, sort_by=[\"category\", \"Krippendorff's alpha\"], col_types={\"n\": int, \"n.1\": int}, to_csv='results/paper/surge_agreements.csv', index=False)\n",
    "agreements"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build the plot\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "def plot_by_category(ax, df, category, color, xaxis_start):\n",
    "    extracted = df[df[\"category\"] == category]\n",
    "    lower_bound = extracted[\"Krippendorff's alpha\"] - extracted[\"CI low\"]\n",
    "    upper_bound = extracted[\"CI high\"] - extracted[\"Krippendorff's alpha\"]\n",
    "    xaxis_end = xaxis_start + len(extracted)\n",
    "    ax.errorbar(np.arange(xaxis_start, xaxis_end),\n",
    "                extracted[\"Krippendorff's alpha\"],\n",
    "                yerr=[lower_bound, upper_bound],\n",
    "                fmt='o',\n",
    "                elinewidth=1,\n",
    "                color=color)\n",
    "    return xaxis_end\n",
    "\n",
    "likert_turn_color = \"blue\"\n",
    "likert_dialogue_color = \"red\"\n",
    "comparative_color = \"green\"\n",
    "behavior_color = \"orange\"\n",
    "\n",
    "likert_dialogue_start = plot_by_category(ax, agreements, \"likert turn\", likert_turn_color, 0)\n",
    "comparative_start = plot_by_category(ax, agreements, \"likert dialogue\", likert_dialogue_color, likert_dialogue_start)\n",
    "behavior_start = plot_by_category(ax, agreements, \"comparative\", comparative_color, comparative_start)\n",
    "misc_start = plot_by_category(ax, agreements, \"behavior\", behavior_color, behavior_start)\n",
    "\n",
    "category_range = {likert_dialogue_start: likert_turn_color, comparative_start: likert_dialogue_color, behavior_start: comparative_color, misc_start: behavior_color}\n",
    "xaxis_colors = {}\n",
    "prev_idx = 0\n",
    "for idx, color in category_range.items():\n",
    "    for i in range(prev_idx, idx):\n",
    "        xaxis_colors[i] = color\n",
    "    prev_idx = idx\n",
    "\n",
    "ax.set_ylabel(\"Krippendorf's alpha\")\n",
    "xpos = np.arange(len(agreements))\n",
    "ax.set_xlabel(\"Evaluation Label\")\n",
    "ax.set_xticks(xpos)\n",
    "ax.set_xticklabels(agreements[\"label\"], rotation=90)\n",
    "for tickloc, ticklabel in zip(plt.gca().get_xticks(), plt.gca().get_xticklabels()):\n",
    "    ticklabel.set_color(xaxis_colors[tickloc])\n",
    "ax.set_title('Interannotator Agreement')\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "# Save the figure and show\n",
    "plt.tight_layout()\n",
    "# plt.savefig('bar_plot_with_error_bars.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_agreements = across_evaluations(\n",
    "    [\n",
    "        e.annotation_dataframe() for e in\n",
    "        (data.student_evaluation, data.mturk_evaluation, data.surge_evaluation)\n",
    "    ],\n",
    "    agreement_dataframe,\n",
    "    load='results/evaluation_agreements.csv'\n",
    ")\n",
    "all_agreements = prettify(all_agreements, float_prec=3, sort_by=[\"round\", \"category\", \"Krippendorff's alpha\"], col_types={\"n\": int, \"n.1\": int}, to_csv='results/paper/all_agreements.csv', index=False)\n",
    "all_agreements"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7 Comprehensive Analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Likert Dialogue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bots = ['Blender2', 'Emora', 'BartFidRAG', 'RerankBlender']\n",
    "# https://blog.finxter.com/how-to-plot-matplotlibs-color-palette-and-choose-your-plot-color/\n",
    "graphing_bot_colors = {\n",
    "    'blender2_3B': 'purple',\n",
    "    'bart_fid_rag_bcb': 'royalblue',\n",
    "    'emora': 'turquoise',\n",
    "    'rerank_blender': 'green'\n",
    "}\n",
    "bot_transformer = {\n",
    "    'blender2_3B': 'Blender2',\n",
    "    'emora': 'Emora',\n",
    "    'rerank_blender': 'Blender-Decode',\n",
    "    'bart_fid_rag_bcb': 'BART-FiDRAG'\n",
    "}\n",
    "dimensions_transformer = {\n",
    "    'consistent': 'CO',\n",
    "    'emotional': 'EU',\n",
    "    'engaging': 'EN',\n",
    "    'grammatical': 'GR',\n",
    "    'informative': 'IN',\n",
    "    'proactive': 'PR',\n",
    "    'quality': 'OQ',\n",
    "    'relevant': 'RE'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def grouped_barplot(df, title, ylabel, xlabel, ylim, value_col='mean', rot=45, fig_size=(10,5)):\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "    df['lower'] = df[value_col] - df[\"CI low\"]\n",
    "    cilow = df.pivot(index='label', columns='bot', values='lower')\n",
    "    df['upper'] = df[\"CI high\"] - df[value_col]\n",
    "    cihigh = df.pivot(index='label', columns='bot', values='upper')\n",
    "\n",
    "    err = []\n",
    "    for col in cilow:\n",
    "        err.append([cilow[col].values, cihigh[col].values])\n",
    "\n",
    "    df0 = df.pivot(index='label', columns='bot', values=value_col)\n",
    "    ax = df0.plot(\n",
    "        kind='bar',\n",
    "        ylim=ylim,\n",
    "        title=title,\n",
    "        rot=rot,\n",
    "        yerr=err,\n",
    "        color=[graphing_bot_colors[bot] for bot in df0.columns]\n",
    "    )\n",
    "    ax.legend(\n",
    "        [bot_transformer[bot] for bot in df0.columns],\n",
    "        ncol=2\n",
    "    )\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_xticklabels([dimensions_transformer[d] if d in dimensions_transformer else behaviors_transformer[d] for d in df0.index])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_likert_ratings(annotations, category, load=None, reload=None):\n",
    "    if load:\n",
    "        return pd.read_csv(load)\n",
    "    single_annotated = get_singly_annotated(annotations)\n",
    "    likert_annotations = single_annotated.xs(category, level=sym.category)\n",
    "    label_groups = likert_annotations.groupby(level=[sym.bot, sym.label])\n",
    "    means = label_groups.apply(mean_and_ci)\n",
    "    if reload:\n",
    "        means.to_csv(reload)\n",
    "    return means"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "surge_likert_dialogue_ratings = evaluate_likert_ratings(\n",
    "    surge_annotations, category.likert_dialogue,\n",
    "    load='results/surge_likert_dialogue_ratings.csv'\n",
    ")\n",
    "sldr = prettify(surge_likert_dialogue_ratings, float_prec=3, col_types={\"n\": int}, sort_by=[\"bot\", \"mean\"], to_csv=\"results/paper/surge_likert_dialogue_ratings.csv\", index=False)\n",
    "sldr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grouped_barplot(sldr, title=\"Average Dialogue Likert Rating\", ylabel=\"Likert Rating\", xlabel='Label', ylim=(2.5,4.55), rot=0, fig_size=(10,3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Likert Turn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "surge_likert_turn_ratings = evaluate_likert_ratings(\n",
    "    surge_annotations, category.likert_turn,\n",
    "    load='results/surge_likert_turn_ratings.csv'\n",
    ")\n",
    "sltr = prettify(surge_likert_turn_ratings, float_prec=3, col_types={\"n\": int}, sort_by=[\"bot\", \"mean\"], to_csv=\"results/paper/surge_likert_turn_ratings.csv\", index=False)\n",
    "sltr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grouped_barplot(sltr, title=\"Average Turn Likert Rating\", ylabel=\"Likert Rating\", xlabel='Label', ylim=(2.5,5.0), rot=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparative"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "comparison_df = evaluate_comparisons(\n",
    "    surge_annotations_comparative,\n",
    "    reload='results/surge_comparisons.csv'\n",
    ")\n",
    "comparison_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# each bot is a dataframe\n",
    "botvothers = comparison_df[comparison_df.index.get_level_values('bot comp') == 'others'][['win', 'tie', 'lose']]\n",
    "print(comparison_df.columns)\n",
    "botvothers['CI low'] = comparison_df.iloc[:, 9]\n",
    "botvothers['CI high'] = comparison_df.iloc[:, 10]\n",
    "botvothers.reset_index(level=['bot comp'], inplace=True)\n",
    "botvothers.drop('bot comp', inplace=True, axis='columns')\n",
    "toplot = botvothers.reorder_levels(['label', 'bot']).sort_index()\n",
    "toplot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib.text import Text\n",
    "\n",
    "def plot_comparative(df0, title, value_col, fig_size):\n",
    "    # https://stackoverflow.com/questions/59922701/pandas-how-can-i-group-a-stacked-bar-chart\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "    df0['lower'] = df0[value_col] - df0[\"CI low\"]\n",
    "    df0['upper'] = df0[\"CI high\"] - df0[value_col]\n",
    "\n",
    "    errLow = df0[['lower']].reset_index(['bot', 'label']).pivot(index='label', columns='bot', values='lower')\n",
    "    errHi = df0[['upper']].reset_index(['bot', 'label']).pivot(index='label', columns='bot', values='upper')\n",
    "\n",
    "    # 4 x 2 x 8 (bots x low, hi x labels)\n",
    "    err = []\n",
    "    for col in errLow:\n",
    "        err.append([errLow[col].values, errHi[col].values])\n",
    "\n",
    "    df0 = df0.unstack(level=-1)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    groups = []\n",
    "    for i in df0.columns:\n",
    "        if i[1] not in groups:\n",
    "            groups.append(i[1])\n",
    "\n",
    "    (df0['win']+df0['tie']+df0['lose']).plot(kind='bar', color=[graphing_bot_colors[i] for i in groups], alpha=0.2, rot=0, ax=ax)\n",
    "    (df0['win']+df0['tie']).plot(kind='bar', color=[graphing_bot_colors[i] for i in groups], alpha=0.4, rot=0, ax=ax)\n",
    "    df0['win'].plot(kind='bar', color=[graphing_bot_colors[i] for i in groups], rot=0, ax=ax, yerr=err)\n",
    "\n",
    "    h, l = ax.get_legend_handles_labels()\n",
    "    markers = {}\n",
    "    for h, l, (wtl, bot) in zip(h, l, df0.columns):\n",
    "        markers.setdefault(bot, []).append((h,l))\n",
    "    wtl_dummies = [plt.plot([],marker=\"\", ls=\"\")[0]]*4\n",
    "    bot_dummies = [plt.plot([],marker=\"\", ls=\"\")[0]]*4\n",
    "    handles = wtl_dummies\n",
    "    labels = [\"\", \"Lose:\", \"Tie:\", \"Win:\"]\n",
    "    for i, (bot, symbols) in enumerate(markers.items()):\n",
    "        handles.append(bot_dummies[i])\n",
    "        labels.append(bot_transformer[bot])\n",
    "        handles.extend([s[0] for s in symbols])\n",
    "        labels.extend([\"\" for s in symbols])\n",
    "    leg = plt.legend(handles, labels, ncol=5, loc='upper right', bbox_to_anchor=(0.67, -0.35), labelspacing=0.25)\n",
    "    for i, vpack in enumerate(leg._legend_handle_box.get_children()):\n",
    "        if i == 0: # row titles\n",
    "            for hpack in vpack.get_children():\n",
    "                hpack.get_children()[0].set_width(0)\n",
    "        else:\n",
    "            for j, hpack in enumerate(vpack.get_children()):\n",
    "                if j > 0: # bot win/tie/lose markers\n",
    "                    hpack.get_children()[0].get_children()[0].set_width(50)\n",
    "                else: # column titles\n",
    "                    hpack.get_children()[0].set_width(0)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Proportion')\n",
    "    ax.set_xlabel('Label')\n",
    "    ax.set_xticklabels([dimensions_transformer[d] for d in df0.index])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return df0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = plot_comparative(toplot, 'Comparative Evaluation Results', 'win', (15,5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Behaviors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_behavior_rates(annotations, load=None, reload=None):\n",
    "    if load:\n",
    "        return pd.read_csv(load)\n",
    "    single_annotated = get_singly_annotated(annotations)\n",
    "    behavior_annotations = single_annotated.xs(category.behavior, level=sym.category)\n",
    "    label_groups = behavior_annotations.groupby(level=[sym.bot, sym.label])\n",
    "    means = label_groups.apply(prop_and_ci)\n",
    "    if reload:\n",
    "        means.to_csv(reload)\n",
    "    return means"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "surge_behavior_rates = evaluate_behavior_rates(\n",
    "    surge_annotations,\n",
    "    load='results/surge_behavior_rates.csv'\n",
    ")\n",
    "sbr = prettify(surge_behavior_rates,  float_prec=3, col_types={\"n\": int}, sort_by=[\"bot\", \"proportion\"], to_csv=\"results/paper/surge_behavior_rates.csv\", index=False)\n",
    "sbr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "behaviors_transformer = {\n",
    "    'correct fact': 'CF',\n",
    "    'empathetic': 'EM',\n",
    "    'follow up': 'FU',\n",
    "    'life info': 'LI',\n",
    "    'preference info': 'PI',\n",
    "    'uninterpretable': 'UI',\n",
    "    'antisocial': 'AS',\n",
    "    'commonsense contradiction': 'CC',\n",
    "    'ignore': 'IG',\n",
    "    'incorrect fact': '~CF',\n",
    "    'irrelevant': 'IR',\n",
    "    'lack of empathy': '~EM',\n",
    "    'partner contradiction': 'PC',\n",
    "    'redundant': 'RD',\n",
    "    'self contradiction': 'SC',\n",
    "    'topic switch': 'TS'\n",
    "}\n",
    "\n",
    "to_maximize = {'correct fact', 'empathetic', 'follow up', 'life info', 'preference info'}\n",
    "maximize = sbr[sbr['label'].isin(to_maximize)]\n",
    "grouped_barplot(maximize, title=\"Rates of Desirable Behaviors\", ylabel=\"Behavior\", xlabel='Label', ylim=(0,0.7), value_col=\"proportion\", rot=0, fig_size=(20,5))\n",
    "\n",
    "to_minimize = {'uninterpretable', 'antisocial', 'commonsense contradiction', 'ignore', 'incorrect fact', 'irrelevant', 'lack of empathy', 'partner contradiction', 'redundant', 'self contradiction', 'topic switch'}\n",
    "minimize = sbr[sbr['label'].isin(to_minimize)]\n",
    "grouped_barplot(minimize, title=\"Rates of Undesirable Behaviors\", ylabel=\"Behavior\", xlabel='Label', ylim=(0,0.35), value_col=\"proportion\", rot=0, fig_size=(20,5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8 Evaluation Metric Assessment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Metric Sensitivity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "outputs": [
    {
     "data": {
      "text/plain": "              blender2_3B                                          emora  \\\n                    emora rerank_blender bart_fid_rag_bcb rerank_blender   \nlabel                                                                      \nconsistent   3.202258e-01   3.974450e-01     1.641548e-03   6.150188e-02   \nemotional    1.458590e-12   4.873241e-01     8.042664e-10   3.089800e-10   \nengaging     3.388186e-02   5.737756e-05     1.548385e-13   4.450630e-02   \ngrammatical  1.259973e-05   8.340986e-83     9.079602e-30   5.853912e-61   \ninformative  2.358728e-10   7.321290e-15     2.114359e-20   1.607407e-40   \nproactive    1.599599e-05   8.555421e-02     8.080369e-51   1.403651e-02   \nquality      4.717359e-08   4.606636e-27     2.810324e-22   1.950697e-07   \nrelevant     8.285739e-17   7.896203e-23     2.426902e-09   1.561723e-01   \n\n                               rerank_blender  \n            bart_fid_rag_bcb bart_fid_rag_bcb  \nlabel                                          \nconsistent      3.082214e-05     1.828072e-02  \nemotional       5.902241e-01     6.072199e-08  \nengaging        4.034663e-08     5.716060e-04  \ngrammatical     1.514652e-15     1.987924e-16  \ninformative     9.844346e-49     1.410483e-01  \nproactive       2.465946e-78     7.902666e-58  \nquality         9.602646e-06     5.985842e-01  \nrelevant        3.532057e-02     4.657267e-04  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"3\" halign=\"left\">blender2_3B</th>\n      <th colspan=\"2\" halign=\"left\">emora</th>\n      <th>rerank_blender</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>emora</th>\n      <th>rerank_blender</th>\n      <th>bart_fid_rag_bcb</th>\n      <th>rerank_blender</th>\n      <th>bart_fid_rag_bcb</th>\n      <th>bart_fid_rag_bcb</th>\n    </tr>\n    <tr>\n      <th>label</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>consistent</th>\n      <td>3.202258e-01</td>\n      <td>3.974450e-01</td>\n      <td>1.641548e-03</td>\n      <td>6.150188e-02</td>\n      <td>3.082214e-05</td>\n      <td>1.828072e-02</td>\n    </tr>\n    <tr>\n      <th>emotional</th>\n      <td>1.458590e-12</td>\n      <td>4.873241e-01</td>\n      <td>8.042664e-10</td>\n      <td>3.089800e-10</td>\n      <td>5.902241e-01</td>\n      <td>6.072199e-08</td>\n    </tr>\n    <tr>\n      <th>engaging</th>\n      <td>3.388186e-02</td>\n      <td>5.737756e-05</td>\n      <td>1.548385e-13</td>\n      <td>4.450630e-02</td>\n      <td>4.034663e-08</td>\n      <td>5.716060e-04</td>\n    </tr>\n    <tr>\n      <th>grammatical</th>\n      <td>1.259973e-05</td>\n      <td>8.340986e-83</td>\n      <td>9.079602e-30</td>\n      <td>5.853912e-61</td>\n      <td>1.514652e-15</td>\n      <td>1.987924e-16</td>\n    </tr>\n    <tr>\n      <th>informative</th>\n      <td>2.358728e-10</td>\n      <td>7.321290e-15</td>\n      <td>2.114359e-20</td>\n      <td>1.607407e-40</td>\n      <td>9.844346e-49</td>\n      <td>1.410483e-01</td>\n    </tr>\n    <tr>\n      <th>proactive</th>\n      <td>1.599599e-05</td>\n      <td>8.555421e-02</td>\n      <td>8.080369e-51</td>\n      <td>1.403651e-02</td>\n      <td>2.465946e-78</td>\n      <td>7.902666e-58</td>\n    </tr>\n    <tr>\n      <th>quality</th>\n      <td>4.717359e-08</td>\n      <td>4.606636e-27</td>\n      <td>2.810324e-22</td>\n      <td>1.950697e-07</td>\n      <td>9.602646e-06</td>\n      <td>5.985842e-01</td>\n    </tr>\n    <tr>\n      <th>relevant</th>\n      <td>8.285739e-17</td>\n      <td>7.896203e-23</td>\n      <td>2.426902e-09</td>\n      <td>1.561723e-01</td>\n      <td>3.532057e-02</td>\n      <td>4.657267e-04</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def t_tests(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    :param df: (bot, data point) x 1 -> score\n",
    "    :return: p values of test on each bot pair (pd.Series)\n",
    "    \"\"\"\n",
    "    bots = set(df.index.get_level_values(0))\n",
    "    bot_pairs = list(combinations(bots, 2))\n",
    "    result = {}\n",
    "    for ba, bb in bot_pairs:\n",
    "        a = df.xs(ba).to_numpy().squeeze()\n",
    "        b = df.xs(bb).to_numpy().squeeze()\n",
    "        t, p = ttest_ind(a, b)\n",
    "        result[(ba, bb)] = p\n",
    "    result_series = pd.Series(result.values(), result)\n",
    "    return result_series\n",
    "\n",
    "get_singly_annotated(surge_annotations).xs(\n",
    "    category.likert_turn,\n",
    "    level=sym.category\n",
    ").groupby(\n",
    "    sym.label\n",
    ").apply(\n",
    "    t_tests\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predictive Validity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "outputs": [
    {
     "data": {
      "text/plain": "           category                      label       score  \\\n0       likert turn                 consistent    0.437186   \n1       likert turn                  emotional    0.459415   \n2       likert turn                   engaging    0.364656   \n3       likert turn                grammatical   -0.038475   \n4       likert turn                informative   -0.024559   \n5       likert turn                  proactive    0.418151   \n6       likert turn                    quality    0.451505   \n7       likert turn                   relevant    0.411045   \n8          behavior                 antisocial    6.022125   \n9          behavior  commonsense contradiction   -3.765228   \n10         behavior               correct fact   -0.038283   \n11         behavior                 empathetic    1.637197   \n12         behavior                  follow up    0.507137   \n13         behavior                     ignore   -3.753254   \n14         behavior             incorrect fact   -1.406491   \n15         behavior                 irrelevant   -2.327845   \n16         behavior            lack of empathy   -2.699847   \n17         behavior                  life info    0.487967   \n18         behavior      partner contradiction   -5.722676   \n19         behavior            preference info    0.183520   \n20         behavior                  redundant   -3.761563   \n21         behavior         self contradiction   -3.830152   \n22         behavior               topic switch    0.036299   \n23         behavior            uninterpretable   -7.401468   \n24  likert dialogue                 consistent    0.810984   \n25  likert dialogue                  emotional    0.947239   \n26  likert dialogue                   engaging    1.568202   \n27  likert dialogue                grammatical    0.592631   \n28  likert dialogue                informative    0.988051   \n29  likert dialogue                  proactive    1.101919   \n30  likert dialogue                    quality  112.601055   \n31  likert dialogue                   relevant    1.183381   \n\n    McFadden's pseudo-R-squared  \n0                      0.008301  \n1                      0.010492  \n2                      0.009000  \n3                      0.000054  \n4                      0.000033  \n5                      0.011262  \n6                      0.009052  \n7                      0.010554  \n8                      0.001190  \n9                      0.024278  \n10                     0.000005  \n11                     0.012622  \n12                     0.001516  \n13                     0.015435  \n14                     0.003978  \n15                     0.009423  \n16                     0.013486  \n17                     0.000627  \n18                     0.023771  \n19                     0.000149  \n20                     0.007834  \n21                     0.009924  \n22                     0.000003  \n23                     0.004976  \n24                     0.103444  \n25                     0.098235  \n26                     0.195030  \n27                     0.033306  \n28                     0.078316  \n29                     0.127203  \n30                     1.000000  \n31                     0.128773  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category</th>\n      <th>label</th>\n      <th>score</th>\n      <th>McFadden's pseudo-R-squared</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>likert turn</td>\n      <td>consistent</td>\n      <td>0.437186</td>\n      <td>0.008301</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>likert turn</td>\n      <td>emotional</td>\n      <td>0.459415</td>\n      <td>0.010492</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>likert turn</td>\n      <td>engaging</td>\n      <td>0.364656</td>\n      <td>0.009000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>likert turn</td>\n      <td>grammatical</td>\n      <td>-0.038475</td>\n      <td>0.000054</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>likert turn</td>\n      <td>informative</td>\n      <td>-0.024559</td>\n      <td>0.000033</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>likert turn</td>\n      <td>proactive</td>\n      <td>0.418151</td>\n      <td>0.011262</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>likert turn</td>\n      <td>quality</td>\n      <td>0.451505</td>\n      <td>0.009052</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>likert turn</td>\n      <td>relevant</td>\n      <td>0.411045</td>\n      <td>0.010554</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>behavior</td>\n      <td>antisocial</td>\n      <td>6.022125</td>\n      <td>0.001190</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>behavior</td>\n      <td>commonsense contradiction</td>\n      <td>-3.765228</td>\n      <td>0.024278</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>behavior</td>\n      <td>correct fact</td>\n      <td>-0.038283</td>\n      <td>0.000005</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>behavior</td>\n      <td>empathetic</td>\n      <td>1.637197</td>\n      <td>0.012622</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>behavior</td>\n      <td>follow up</td>\n      <td>0.507137</td>\n      <td>0.001516</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>behavior</td>\n      <td>ignore</td>\n      <td>-3.753254</td>\n      <td>0.015435</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>behavior</td>\n      <td>incorrect fact</td>\n      <td>-1.406491</td>\n      <td>0.003978</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>behavior</td>\n      <td>irrelevant</td>\n      <td>-2.327845</td>\n      <td>0.009423</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>behavior</td>\n      <td>lack of empathy</td>\n      <td>-2.699847</td>\n      <td>0.013486</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>behavior</td>\n      <td>life info</td>\n      <td>0.487967</td>\n      <td>0.000627</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>behavior</td>\n      <td>partner contradiction</td>\n      <td>-5.722676</td>\n      <td>0.023771</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>behavior</td>\n      <td>preference info</td>\n      <td>0.183520</td>\n      <td>0.000149</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>behavior</td>\n      <td>redundant</td>\n      <td>-3.761563</td>\n      <td>0.007834</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>behavior</td>\n      <td>self contradiction</td>\n      <td>-3.830152</td>\n      <td>0.009924</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>behavior</td>\n      <td>topic switch</td>\n      <td>0.036299</td>\n      <td>0.000003</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>behavior</td>\n      <td>uninterpretable</td>\n      <td>-7.401468</td>\n      <td>0.004976</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>likert dialogue</td>\n      <td>consistent</td>\n      <td>0.810984</td>\n      <td>0.103444</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>likert dialogue</td>\n      <td>emotional</td>\n      <td>0.947239</td>\n      <td>0.098235</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>likert dialogue</td>\n      <td>engaging</td>\n      <td>1.568202</td>\n      <td>0.195030</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>likert dialogue</td>\n      <td>grammatical</td>\n      <td>0.592631</td>\n      <td>0.033306</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>likert dialogue</td>\n      <td>informative</td>\n      <td>0.988051</td>\n      <td>0.078316</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>likert dialogue</td>\n      <td>proactive</td>\n      <td>1.101919</td>\n      <td>0.127203</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>likert dialogue</td>\n      <td>quality</td>\n      <td>112.601055</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>likert dialogue</td>\n      <td>relevant</td>\n      <td>1.183381</td>\n      <td>0.128773</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "\n",
    "def regressions(df, quality_column_name=None):\n",
    "    \"\"\"\n",
    "    :param df: dialogue x (*features, quality) -> value\n",
    "    :return: *(coef, low, high), mcfadden r^2\n",
    "    \"\"\"\n",
    "    if not quality_column_name:\n",
    "        quality_column_name = df.columns[-1]\n",
    "    qualities = df[quality_column_name]\n",
    "    features = [f for f in df.columns if f != quality_column_name]\n",
    "    model = OrderedModel(qualities, df[features], distr='logit')\n",
    "    results = model.fit()\n",
    "    coefs = {f: results.params[f] for f in features}\n",
    "    prsqrd = results.prsquared\n",
    "    result = {**coefs, stat.mcfad_r2: prsqrd}\n",
    "    return pd.Series(result.values(), result)\n",
    "\n",
    "def dialogue_metrics(ev):\n",
    "    df: pd.DataFrame = ev.annotation_dataframe()\n",
    "    df = get_singly_annotated(df, seed=123)\n",
    "    reindexed = df.reset_index()\n",
    "    items = reindexed[sym.item]\n",
    "    dialogues = [e[0] if isinstance(e, tuple) else e for e in items]\n",
    "    reindexed['dialogue'] = dialogues\n",
    "    reindexed.set_index(\n",
    "        [sym.bot, sym.category, sym.label, 'dialogue', sym.item],\n",
    "        inplace=True, verify_integrity=True\n",
    "    )\n",
    "    ld = reindexed.xs(category.likert_dialogue, level=sym.category)\n",
    "    ld = ld.droplevel(sym.bot).droplevel(sym.item)\n",
    "    ld.columns = ['score']\n",
    "    ldq = ld.xs(scale.quality, level=sym.label)\n",
    "    ldq.columns = ['quality']\n",
    "\n",
    "    lt = reindexed.xs(category.likert_turn, level=sym.category)\n",
    "    lt = lt.groupby([sym.label, 'dialogue']).mean()\n",
    "    lt.columns = ['score']\n",
    "    ltq = lt.xs(scale.quality, level=sym.label)\n",
    "    ltq.columns = ['quality']\n",
    "\n",
    "    be = reindexed.xs(category.behavior, level=sym.category)\n",
    "    be = be.groupby([sym.label, 'dialogue']).mean()\n",
    "    be.columns = ['score']\n",
    "\n",
    "    ds = pd.concat(\n",
    "        [lt, be, ld],\n",
    "        keys=[category.likert_turn, category.behavior, category.likert_dialogue],\n",
    "        names=[sym.category, sym.label, 'dialogue']\n",
    "    )\n",
    "    likert_dialogue_quality_features = ds.join(ldq, on='dialogue')\n",
    "    likert_turn_quality_features = ds.join(ltq, on='dialogue')\n",
    "    return likert_dialogue_quality_features, likert_turn_quality_features\n",
    "\n",
    "@to_file\n",
    "def dialogue_quality_regressions(ev):\n",
    "    ldq, ltq = dialogue_metrics(ev)\n",
    "    groups = ldq.groupby(\n",
    "        [sym.category, sym.label]\n",
    "    )\n",
    "    result = groups.apply(regressions)\n",
    "    return result\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "dialogue_quality_regressions(\n",
    "    data.surge_evaluation,\n",
    "    load='results/dialogue_quality_regressions.csv'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Agreement Between Static and Interactive Evaluators"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}